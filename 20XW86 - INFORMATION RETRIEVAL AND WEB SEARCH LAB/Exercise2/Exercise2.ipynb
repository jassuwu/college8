{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d6c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = [\n",
    "    'information requirement:query considers the user feedback as information requirement to search',\n",
    "    'information retrieval:query depends on the model of information retrieval used',\n",
    "    'prediction problem:Many problems in information retrieval can be viewed as prediction problems',\n",
    "    'search:A search engine is one of applications of information retrieval models'\n",
    "]\n",
    "\n",
    "docsToAdd = [\n",
    "    'Feedback:feedback is typically used by the system to modify the query and improve prediction',\n",
    "    'information retrieval:ranking in information retrieval algorithms depends on user query',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e375a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### A) Verify if the titles are exactly same (Apply BinaryDistance(u,v), which gives the\n",
    "# binary distance between vectors u and v, equal to 0 if they are identical and 1\n",
    "# otherwise.). If same, label the document as duplicate and discard it else proceed to\n",
    "# second part of the Checker.\n",
    "\n",
    "dbDocTitles = [doc.split(':')[0] for doc in db]\n",
    "\n",
    "for doc in docsToAdd:\n",
    "   if doc.split(':')[0] not in dbDocTitles:\n",
    "    db.append(doc)\n",
    "    dbDocTitles.append(doc.split(':')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4179d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/\n",
    "\n",
    "# Term Frequency\n",
    "def tf(t, d):\n",
    "    return d.split().count(t)/len(d.split())\n",
    "\n",
    "# Document Frequency\n",
    "def df(t):\n",
    "    termCount = 0\n",
    "    for doc in db:\n",
    "        termCount += doc.split(':')[1].split().count(t)\n",
    "    return termCount\n",
    "\n",
    "# Inverse Document Frequency\n",
    "def mod_idf(t):\n",
    "    # Number of documents containing term t\n",
    "    N = 0\n",
    "    for doc in db:\n",
    "        if t in doc.split(':')[1].split():\n",
    "            N += 1\n",
    "    return math.log((N) / (df(t)))\n",
    "\n",
    "# tf-idf(t, d) = tf(t, d) * idf(t)\n",
    "def tf_mod_idf(t, d):\n",
    "    return tf(t, d) * mod_idf(t)\n",
    "\n",
    "tf_mod_idf('consid', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5cc8027",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docMap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Stop word removal\u001b[39;00m\n\u001b[0;32m     17\u001b[0m stopWordRemovedResult \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title, content \u001b[38;5;129;01min\u001b[39;00m docMap\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     19\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(content)\n\u001b[0;32m     20\u001b[0m     filtered_words \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m w\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m stopWords \u001b[38;5;129;01mand\u001b[39;00m w \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'docMap' is not defined"
     ]
    }
   ],
   "source": [
    "### B) Represent documents (excluding the title) as term document vectors with weight of\n",
    "# a term in a document computed as <the-given-formula>\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "docsMap = {doc.split(':')[0]:doc.split(':')[1] for doc in db}\n",
    "\n",
    "\n",
    "# Stop word removal\n",
    "stopWordRemovedResult = {}\n",
    "for title, content in docMap.items():\n",
    "    words = word_tokenize(content)\n",
    "    filtered_words = [w for w in words if not w.lower() in stopWords and w != '.']\n",
    "    stopWordRemovedResult[title] = filtered_words\n",
    "    \n",
    "# Stemming\n",
    "porter = PorterStemmer()\n",
    "stemmedListMap = {}\n",
    "for title, wordList in stopWordRemovedResult.items():\n",
    "    stemmedWords = []\n",
    "    for word in wordList:\n",
    "        stemmedWord = porter.stem(word)\n",
    "        stemmedWords.append(stemmedWord)\n",
    "    stemmedWordMap[title] = stemmedWords\n",
    "print(stemmedWordMap)\n",
    "# # Processed docs\n",
    "# processedWords = []\n",
    "# for stemmedWords in stemmedWordLists:\n",
    "#     processedWords.append(' '.join(stemmedWords))\n",
    "\n",
    "# tfidf = TfidfVectorizer()\n",
    "# result = tfidf.fit_transform(processedWords)\n",
    "\n",
    "# df = pd.DataFrame(result.toarray())\n",
    "# df['title'] = dbDocTitles\n",
    "# df.set_index('title', inplace=True)\n",
    "# df.columns = [list(tfidf.vocabulary_.keys())[list(tfidf.vocabulary_.values()).index(idx)] for idx in df.columns]\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0b71b",
   "metadata": {},
   "source": [
    "### C )\n",
    "\n",
    "https://www.geeksforgeeks.org/measuring-the-document-similarity-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b1c29f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['queri', 'consid', 'user', 'feedback', 'inform', 'requir', 'search'],\n",
       " ['queri', 'depend', 'model', 'inform', 'retriev', 'use'],\n",
       " ['mani', 'problem', 'inform', 'retriev', 'view', 'predict', 'problem'],\n",
       " ['search', 'engin', 'one', 'applic', 'inform', 'retriev', 'model'],\n",
       " ['feedback',\n",
       "  'typic',\n",
       "  'use',\n",
       "  'system',\n",
       "  'modifi',\n",
       "  'queri',\n",
       "  'improv',\n",
       "  'predict']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmedWordLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "350d26ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "information requirement and information retrieval 0.18143953217459566 OG\n",
      "information requirement and prediction problem 0.04172790892816653 OG\n",
      "information requirement and search 0.19107430207450832 OG\n",
      "information requirement and Feedback 0.1930311007999294 OG\n",
      "information retrieval and information requirement 0.18143953217459566 OG\n",
      "information retrieval and prediction problem 0.12088337198929326 OG\n",
      "information retrieval and search 0.3356369614834718 OG\n",
      "information retrieval and Feedback 0.23173639143252944 OG\n",
      "prediction problem and information requirement 0.04172790892816653 OG\n",
      "prediction problem and information retrieval 0.12088337198929326 OG\n",
      "prediction problem and search 0.10069307724718948 OG\n",
      "prediction problem and Feedback 0.07614115136231855 OG\n",
      "search and information requirement 0.19107430207450832 OG\n",
      "search and information retrieval 0.3356369614834718 OG\n",
      "search and prediction problem 0.10069307724718948 OG\n",
      "search and Feedback 0.0 OG\n",
      "Feedback and information requirement 0.1930311007999294 OG\n",
      "Feedback and information retrieval 0.23173639143252944 OG\n",
      "Feedback and prediction problem 0.07614115136231855 OG\n",
      "Feedback and search 0.0 OG\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "titleAndWordList = {\n",
    "    'information requirement': ['queri', 'consid', 'user', 'feedback', 'inform', 'requir', 'search'],\n",
    "    'information retrieval': ['queri', 'depend', 'model', 'inform', 'retriev', 'use'],\n",
    "    'prediction problem': ['mani', 'problem', 'inform', 'retriev', 'view', 'predict', 'problem'],\n",
    "    'search': ['search', 'engin', 'one', 'applic', 'inform', 'retriev', 'model'],\n",
    "    'Feedback': ['feedback', 'typic', 'use', 'system', 'modifi', 'queri', 'improv', 'predict']\n",
    "}\n",
    "\n",
    "def dotProd(A, B):\n",
    "    s = 0\n",
    "    for word in A[1]:\n",
    "        if word in B[1]:\n",
    "            s += (df[word][A[0]] * df[word][B[0]])\n",
    "    return s\n",
    "\n",
    "def cosineSimilarity(D1, D2):  \n",
    "    numerator = dotProd(D1, D2) \n",
    "    denominator = math.sqrt(dotProd(D1, D1)*dotProd(D2, D2)) \n",
    "    return numerator / denominator\n",
    "\n",
    "for docA in titleAndWordList.items():\n",
    "    for docB in titleAndWordList.items():\n",
    "        if docA[0] != docB[0]:\n",
    "            res = cosineSimilarity(docA, docB)\n",
    "            print(f\"{docA[0]} and {docB[0]}\", res, \"PLAIGARISED\" if res > 0.85 else \"OG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163811ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
