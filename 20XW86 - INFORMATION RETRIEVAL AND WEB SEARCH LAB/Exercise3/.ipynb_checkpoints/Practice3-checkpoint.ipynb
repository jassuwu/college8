{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735f32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92521e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing\n",
    "\n",
    "# Documents\n",
    "with open('TIME.ALL', 'r') as f:\n",
    "    text = f.read()\n",
    "result = re.findall(r\"\\*TEXT\\s+\\d{3}\\s+\\d{2}/\\d{2}(/|\\s)\\d{2}\\s+PAGE\\s+\\d{3}\\n\\n(.+?)(?=\\*TEXT|$)\", text, re.DOTALL)\n",
    "docDB = pd.DataFrame(result)\n",
    "docDB.drop(0, axis=1, inplace=True)\n",
    "\n",
    "# Queries\n",
    "with open('TIME.QUE', 'r') as f:\n",
    "    text = f.read()\n",
    "result = re.findall(r'FIND\\s+\\d+\\s+(.*)', text)\n",
    "queryDF = pd.DataFrame(result)\n",
    "\n",
    "# Stopwords\n",
    "with open('TIME.STP','r') as f:\n",
    "    text = f.read()\n",
    "result = re.findall(r\"^[A-Z]+$\", text, re.MULTILINE)\n",
    "swDF = pd.DataFrame(result)\n",
    "\n",
    "# Relevant docs\n",
    "with open('TIME.REL', 'r') as f:\n",
    "    text = f.read()\n",
    "    lines = text.split(\"\\n\")\n",
    "rdDict = {}\n",
    "for line in lines:\n",
    "    numbers = re.findall(r\"\\d+\", line)\n",
    "    if numbers:\n",
    "        key = numbers[0]\n",
    "        values = [int(n) for n in numbers[1:]]\n",
    "        rdDict[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82923a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Term Frequency\n",
    "def tf(t, d):\n",
    "    return d.split().count(t)/len(d.split())\n",
    "\n",
    "# Document Frequency\n",
    "def docfreq(t):\n",
    "    termCount = 0\n",
    "    for doc in docDB[1]:\n",
    "        termCount += doc.split().count(t)\n",
    "    return termCount\n",
    "\n",
    "# Modified Inverse Document Frequency\n",
    "def mod_idf(t):\n",
    "    # Number of documents containing term t\n",
    "    N = 0\n",
    "    for doc in docDB[1]:\n",
    "        if t in doc.split():\n",
    "            N += 1\n",
    "    return math.log((N + 1) / (0.5 + docfreq(t)))\n",
    "\n",
    "# Given weight formula\n",
    "# tf-idf(t, d) = tf(t, d) * mod_idf(t)\n",
    "def tf_mod_idf(t, d):\n",
    "    return tf(t, d) * mod_idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d7432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Has to be run during the first run in a new env\n",
    "# import nltk \n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed6499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "docsMap = {str(idx):doc for idx, doc in enumerate(docDB[1])}\n",
    "\n",
    "# Stop word removal\n",
    "stopWordRemovedResult = {}\n",
    "for title, content in docsMap.items():\n",
    "    words = word_tokenize(content)\n",
    "    filtered_words = [w for w in words if not w.lower() in stopWords and w != '.']\n",
    "    stopWordRemovedResult[title] = filtered_words\n",
    "stopWordRemovedResult\n",
    "    \n",
    "# Stemming\n",
    "porter = PorterStemmer()\n",
    "stemmedListMap = {}\n",
    "for title, wordList in stopWordRemovedResult.items():\n",
    "    stemmedWords = []\n",
    "    for word in wordList:\n",
    "        stemmedWord = porter.stem(word)\n",
    "        stemmedWords.append(stemmedWord)\n",
    "    stemmedListMap[title] = stemmedWords\n",
    "stemmedListMap\n",
    "\n",
    "# Joining\n",
    "stemmedDB = {title: ' '.join(content) for title, content in stemmedListMap.items()}\n",
    "allTerms = sorted(list(set([term for doc in stemmedListMap.values() for term in doc])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6109ca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invertedIndex\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create the inverted index\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m invertedIndex \u001b[38;5;241m=\u001b[39m create_inverted_index(stemmedDB)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(invertedIndex)\n",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m, in \u001b[0;36mcreate_inverted_index\u001b[1;34m(corpus)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m docID, doc \u001b[38;5;129;01min\u001b[39;00m corpus\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[1;32m----> 8\u001b[0m         invertedIndex[term]\u001b[38;5;241m.\u001b[39mappend((docID, tf_mod_idf(term, doc)))\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m invertedIndex\n",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m, in \u001b[0;36mtf_mod_idf\u001b[1;34m(t, d)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtf_mod_idf\u001b[39m(t, d):\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf(t, d) \u001b[38;5;241m*\u001b[39m mod_idf(t)\n",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m, in \u001b[0;36mmod_idf\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     16\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docDB[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m     19\u001b[0m         N \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mlog((N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m+\u001b[39m docfreq(t)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict \n",
    "\n",
    "# Define a function to create an inverted index for the corpus\n",
    "def create_inverted_index(corpus):\n",
    "    invertedIndex = defaultdict(list)\n",
    "    for docID, doc in corpus.items():\n",
    "        for term in doc:\n",
    "            invertedIndex[term].append((docID, tf_mod_idf(term, doc)))\n",
    "    return invertedIndex\n",
    "# Create the inverted index\n",
    "invertedIndex = create_inverted_index(stemmedDB)\n",
    "print(invertedIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ccacc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
