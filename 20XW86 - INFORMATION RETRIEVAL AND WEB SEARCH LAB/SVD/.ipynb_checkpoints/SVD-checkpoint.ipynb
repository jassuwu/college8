{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735f32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92521e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing\n",
    "# Documents\n",
    "with open('TIME.ALL', 'r') as f:\n",
    "    text = f.read()\n",
    "result = re.findall(r\"\\*TEXT\\s+(\\d{3})\\s+(\\d{2}/\\d{2}(/|\\s)\\d{2})\\s+PAGE\\s+(\\d{3})\\n\\n(.+?)(?=\\*TEXT|$)\", text, re.DOTALL)\n",
    "docDB = {match[0]: {'id': match[0], 'date': match[1], 'page': int(match[3]), 'text': match[4]} for match in result}\n",
    "docOnlyDB = {match[0]: match[4] for match in result}\n",
    "docDF = pd.DataFrame(docDB).T\n",
    "\n",
    "# Queries\n",
    "with open('TIME.QUE', 'r') as f:\n",
    "    text = f.read()\n",
    "queryDB = re.findall(r'FIND\\s+\\d+\\s+(.+?)(?=\\n\\n\\*FIND\\s+\\d+\\s+|$)', text, re.DOTALL)\n",
    "\n",
    "# Stopwords\n",
    "with open('TIME.STP','r') as f:\n",
    "    text = f.read()\n",
    "swDB = re.findall(r\"^[A-Z]+$\", text, re.MULTILINE)\n",
    "swDB = set([word.lower() for word in swDB])\n",
    "\n",
    "# Relevant docs\n",
    "with open('TIME.REL', 'r') as f:\n",
    "    text = f.read()\n",
    "    lines = text.split(\"\\n\")\n",
    "rdDB = {}\n",
    "for line in lines:\n",
    "    numbers = re.findall(r\"\\d+\", line)\n",
    "    if numbers:\n",
    "        key = numbers[0]\n",
    "        values = [int(n) for n in numbers[1:]]\n",
    "        rdDB[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165a506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a document\n",
    "def tokenizeDocument(documentText):\n",
    "    return documentText.split()\n",
    "\n",
    "# Normalize and Stop the token stream of a document\n",
    "def normalizeAndStopTokenStream(tokenStream):\n",
    "    return [token.lower() for token in tokenStream if token.lower().isalnum() and token.lower() not in swDB]\n",
    "\n",
    "# Stem and the normalized token stream of a document\n",
    "def stemNormalizedTokenStream(normalizedTokenStream):\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(token) for token in normalizedTokenStream])\n",
    "\n",
    "def processDocument(documentText):\n",
    "    return stemNormalizedTokenStream(normalizeAndStopTokenStream(tokenizeDocument(documentText)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17f95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedDocDB = {docID: processDocument(text) for docID, text in docOnlyDB.items()}\n",
    "processedDocDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "allTerms = sorted(list(set([token for doc in processedDocDB.values() for token in doc.split()])))\n",
    "allTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0818327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of documents a term appears in the DB\n",
    "def df(term, documentDB):\n",
    "    return len([1 for document in documentDB if term in document])\n",
    "\n",
    "# Inverse document frequency or informativeness of a term\n",
    "def idf(term, documentDB):\n",
    "    return math.log((len(documentDB) + 1) / (df(term, documentDB) + 0.5))\n",
    "\n",
    "# Precalculating and making a cache\n",
    "idfMap = {term:idf(term, processedDocDB.values()) for term in allTerms}\n",
    "def getIDF(term):\n",
    "    return idfMap[term]\n",
    "\n",
    "# Calculate weights for a document\n",
    "def calculateDocumentWeight(document):\n",
    "    counter = collections.Counter(document.split())\n",
    "    return [counter.get(term, 0) * getIDF(term) for term in allTerms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6533779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdMatrixDF = pd.DataFrame([calculateDocumentWeight(processedDoc) for processedDoc in processedDocDB.values()], columns=allTerms, index=processedDocDB.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5602ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processedQueryDB = [processDocument(query) for query in queryDB]\n",
    "processedQueryDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3a700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimiliary(queryWeights, tdMatrix):\n",
    "    return np.dot(tdMatrix, queryWeights) / (np.linalg.norm(queryWeights) * np.linalg.norm(tdMatrix, axis=1))\n",
    "\n",
    "def findTopNRelevantDocsWithCosineSimilarity(query, tdMatrixDF, N):\n",
    "    # Calculate the cosine similarity\n",
    "    cosineSimilarities = cosineSimiliary(calculateDocumentWeight(query), tdMatrixDF.values)\n",
    "\n",
    "    # Sort in descending order of cosine similarity\n",
    "    df = pd.DataFrame({'docID': tdMatrixDF.index, 'cosineSimilarity': cosineSimilarities})\n",
    "    sorted_df = df.sort_values(by='cosineSimilarity', ascending=False)\n",
    "\n",
    "    # Return the top 10 relevant documents from the sorted dataframe\n",
    "    return sorted_df['docID'].values[:N].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineSimiliaryResults = {str(idx + 1):findTopNRelevantDocsWithCosineSimilarity(processedQuery, tdMatrixDF, 10) for idx, processedQuery in enumerate(processedQueryDB)}\n",
    "cosineSimiliaryResults"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a41b3f",
   "metadata": {},
   "source": [
    "# Arranging the TDMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d48cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdMatrixDF = tdMatrixDF.T\n",
    "tdMatrixDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = pd.DataFrame(np.dot(tdMatrixDF.T, tdMatrixDF))\n",
    "V = pd.DataFrame(np.dot(tdMatrixDF, tdMatrixDF.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04330c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(U)\n",
    "singular_values = np.sqrt(eigenvalues)\n",
    "\n",
    "Sigma = np.diag(singular_values)\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbe4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
